
2018 March 23 (Fri)
=================

* Discussed correlation output:
	- Look for literature on problem of dimentionality in ML
* Steve to relook into the AUC of LDA model
* Steve to look for some cutting edge questions in ML and cancer by reviewing some literature
 - Possibly pick Kourou and say something about the paper
* Steve to consider image processing as one of the option in Cancer research
 - Jonathan will contact a friend who has done some work on image processing
* Steve to decide whether he can spend some time writing reviews of some papers on cancer
* If possible, Steve to send email to Jonathan on the progress every monday before next meeting.

2018 Jun 22 (Fri)
=================

We think about an _analysis_ step and a _threshold_ step, and we
should separate these conceptually. Some of the metrics (sensitivity,
specificity) evaluate _after_ the threshold step, and some evaluate
before (AUC). The former are more general.

Some metrics don't even make sense by themselves (for example, we
should never report sensitivity without specificity (or vice versa))

The report in general has too much information. This is possibly
because we don't understand yet which pieces are more important. So we
should understand better, and present less information but more
clearly.

Details:
* what is meant by 100% of the between-group variance?
* what is the cool correlation matrix picture actually showing?
* can we identify a small number of "good" metrics that are not
sensitive to threshold?
* AUC; matching sensitivity with specificity (what is that one
called?); suggest one or two others?
* maybe show boxplot with sens/spec statistic
* clear explanation of boxplot

Resampling methods are good; make sure we can explain these concisely

Future:
* Dashboard style presentation to make it clearer for others

JD: investigate about a relocation advance

Already in canada

2018 Oct 26 (Fri)
=================

* Remake the probability density plot, but code (non-redundantly) by _observed_ diagnosis
	- Modified the problem to: "Given the predicted class 'M', what's the probability that the algorithm correctly predicted the observed?" and the vizualised using boxplot which seems to be more interpretable than density plot.

* Keep working on understanding of algorithms, and particularly parameters (what is tuned and why, what is not tuned and why, what are the defaults or hyper-defaults (default ranges for tuned parameters))

* Better colors (color brewer)

2018 Nov 09 (Fri)
=================

Why is it good to do multi-classification on a binary problem?
* JD says not good, so delete or respond
-- Deleted

Let's figure out how to get neuralnet responses, AUC.
--  Added AUC plots

What is neuralnet really measuring with the cross-entropy? How good are our good values? Is it worth trying again with two layers of neurons, since that sounds fun?
--  Added a function to generate diffent combination of layers and neurons.
We need to find out if cross-entropy is a pure training statistic or if it has some cross-validation. If not, we need to figure out how to do some cross-validation (maybe k-fold?)
-- Added test AUC to compare the two. I will implement cv in the next update.
