---
title: 'Application of Machine Learning Algorithms to Cancer Diagnosis'
author: "Steve and Jonathan"
date: " `r as.Date(Sys.time())` "
output: 
  html_document: 
    number_sections: yes
    toc: yes
    toc_depth: 5
    toc_float: yes
---




```{r setup, include=FALSE, cache=F}
knitr::opts_chunk$set(echo = F, warning = F, message = F, fig.width = 10, fig.height = 10, results = "asis")
options(width = 10)
rm(list=ls())
# Set practice root directory
basedir <- "C:/Users/sbicko/Google Drive/PHD/Practical Exercises/Wisconsin Cancer Dataset/"

knitr::opts_knit$set(root.dir = normalizePath(basedir))
```

# Introduction

Machine Learning (ML) is a science which involves the application of Artificial Intelligence (AI) that enables the computers to automatically learn and improve or get things done based on experience without being explicitly programed. This involves computer looking for patterns based on previous observations, experiences or instructions. In this exercise, we apply some of the Machine Learning Algorithms in Diagnosis of Cancer using [Breast Cancer Wisconsin Data Set](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(original)) and also review some of the papers that have been published analyzing the same data set.


## Wisconsin Diagnostic Breast Cancer (WDBC)

Various ML algorithms, such as Artificial neural networks (ANNs) and decision trees (DTs) have been used in cancer detection and diagnosis. Using [Breast Cancer (WDBC) data set](https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.names), we'll implement these algorithms in analyzing WDBC data set.

The data set contains $569$ cases with $32$ variables. The diagnosis classification is either (M = Malignant) or (B = Benign). Other variables include cell nucleus:

* Radius
* Texture
* Perimeter
* Smoothness
* Compactness
* Concave points
* Symmetry
* Fractal dimension



```{r usefulpackages, echo=T}
# Useful Packages

pkgs <- c("data.table", "Amelia", "stringr", "dplyr", "corrplot", "caret", "MASS", "openxlsx",
          "ROCR", "caTools", "e1071")
if (!"pacman" %in% installed.packages()[,1]){
  install.packages("pacman")
}
pacman::p_load(pkgs, install = T, character.only = T)
```



```{r usefulfunctions, echo=T}
# Functions to be used

factorFunc <- function(x){
  factor(x)
}



# Variables with missing Values
missPropFunc <- function(dat){
  dat <- as.data.frame(dat)
  vars <- apply(dat,2,function(x) round((sum(is.na(x))/length(x))*100, 2))
  miss_vars <- vars[vars>0]
  miss_data <- as.data.frame(miss_vars)
  if (nrow(miss_data)>0){
    return(miss_data)
  } else {
    print("No missing entries")
  }
}

```



We download the data set and description file from the [site](https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data), convert them to \textit{.xlsx} files and then store locally. Illustrated below.

```{r wdbc_df, echo=T}
# Check if Dataset dir exist otherwise R create it.
if(!exists("Datasets")){
  dir.create(file.path(basedir, "Datasets"))
  dir.create(file.path("Datasets", "Out"))
  dir.create(file.path("Datasets", "In"))
  cat("Dataset will be saved in created directory \n", 
      paste(basedir, "Dataset/Out"))
}


# Define data url
df_url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data"
desc_url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.names"

# Check if the dataset already exist
pattern <- "wdbc_data" # How is the dataset named in the computer?
df_path <- paste(basedir, "Datasets/Out/", sep = '')
if(length(list.files(df_path))>0){
  if (grepl(pattern, list.files(df_path), ignore.case = T)){
  df_name <- grep(pattern, list.files(df_path), value = T)
  print("Reading dataset from your computer... \n")
  working_df <- read.xlsx(paste("Datasets/Out/", df_name, sep = ''), sheet = 2)
  cat(df_name, " dataset already saved!!! We'll proceed to modeling.", "\n")
  }
} else {
  
  # Download data
  cat("Dowloading dataset from ", df_url, "\n")
  wdbc_df <- fread(df_url, showProgress=F)
  # Get variable information
  desc_file <- readLines(desc_url)
  var_info_pos <- grep(". Attribute information", desc_file, ignore.case = T)+2
  var_info <- desc_file[var_info_pos:(var_info_pos+5+ncol(wdbc_df)/3)]
  var_info <- var_info[!var_info %in% c("", "Ten real-valued features are computed for each cell nucleus:", "3-32)")]
  var_info <- sub('.*\\\t', '', var_info)
  var_info <- sub('.*\\) \\b', '', var_info)
  var_info_df <- data.frame(vars=var_info)
  var_info_df$labels <- as.character(var_info_df$vars)
  var_info_df$vars <- str_wrap(tolower(sub('\\(.*', '', var_info_df$vars)))
  var_info_df$vars <- sub(" ", "_", var_info_df$vars)
  rep_vars <- var_info_df[3:nrow(var_info_df), ]
  var_info_df <- rbind(var_info_df, rep_vars[rep(seq_len(nrow(rep_vars)), 2), ])
  measurements <- rep(c("mean", "se", "worst"), each=10)
  var_info_df$vars[3:nrow(var_info_df)] <- paste(var_info_df$vars[3:nrow(var_info_df)], measurements, sep = '_')
  measurements <- rep(c("Mean", "Standard error", "Worst"), each=10)
  var_info_df$labels[3:nrow(var_info_df)] <- paste(var_info_df$labels[3:nrow(var_info_df)], measurements, sep = ' - ')
  colnames(wdbc_df) <- var_info_df$vars
  #Add variable names to the data set and save the data set in .xlsx together the variable discription
  wdbc_df_info <- list(Description = var_info_df, wdbc_data = wdbc_df)
  openxlsx::write.xlsx(wdbc_df_info, "Datasets/Out/wdbc_dataset.xlsx") # Uncomment to save the dataset
  working_df <- wdbc_df
  cat(pattern, " didn't exist!!! We've downloaded data from the url ", df_url, "\n Dataset dim: ",
      dim(wdbc_df))

}


```



### Data cleaning and Descriptives

```{r desc, echo=T}
str(working_df)
```

Convert \textbf{diagnosis} to factor and check for missingness.

```{r factor, echo=T}
working_df$diagnosis <- factorFunc(working_df$diagnosis)
missPropFunc(working_df)
```

Summarize numerical variables.

```{r, echo=T}
vars_type <- sapply(working_df, class, simplify = F)
num_vars <- names(grep("numer", vars_type, value = T))
summary_tab <- lapply(working_df[, num_vars], summary)
summary_df <- Reduce(rbind, summary_tab)
summary_df <- apply(summary_df,2, function(x){round(x, 3)})
row.names(summary_df) <- names(summary_tab)
knitr::kable(summary_df)
```


The overall distribution of diagnosis indicators and distribution accross all features:

```{r, echo=T}
knitr::kable(working_df %>% 
  count(diagnosis) %>% 
  mutate(percentage=round(n/sum(n)*100, 2)))
```



```{r, echo=T}
ind_vars <- names(working_df)[!names(working_df) %in% c("diagnosis", "id_number")]
form <- as.formula(paste(ind_vars[1], "~", "diagnosis"))
old.par <- par(mfrow=c(5, 6))
for (i in 1:length(ind_vars)){
 boxplot(form, data = working_df, main=paste("Cancer diagnosis by", ind_vars[i]), xlab="Diagnosis", cex.main=0.8, ylab=ind_vars[i], cex=0.8) 
}
par(old.par)
```


#### Correlation

The next step invloves checking the correlation between the features. This will help us reduce the number of features based on the strength of association. In addition, training a model on a data set with features which have little or no correlation may lead to inaccurate results. It is therefore important to identify and filter out features which are not correlated to other features, more so if measuring the similar aspects, [Yu, Lei and Liu, Huan](http://www.aaai.org/Papers/ICML/2003/ICML03-111.pdf).


```{r, echo=T}
features_df <- working_df[, ind_vars]
cor_mat <- cor(features_df)
corrplot(cor_mat, order = "hclust", tl.cex = 1, addrect = 8)
```


## Model Fitting


### Principal Component Analysis and Linear Discriminant Analysis

Since there are many correlated features, we will use PCA to reduce the dimension of the data. Both LDA and PCA are used to classify and reduce the dimentionality of the data. One of the key difference is that PCA is unsupervised learning while LDA is supervised. 


#### PCA

Principal Component Analysis is one of the fundamental mathematical technique for dimentionality reduction. One of the requirement of a PCA dataset is that it should be scaled. We therefore need to scale the features before applying PCA. We set \textbf{center=TRUE} and \textbf{scale=TRUE}.

```{r, echo=T}
pca_result <- prcomp(working_df[, ind_vars], center = TRUE, scale = TRUE)
biplot(pca_result, scale = 0)
```


To access the number of components which would explain much of the variations, we use a scree plot.

```{r, echo=T}
# Compute the variance of each componet
pca_var <- (pca_result$sdev)^2
# Proportion of variance explained
prop_var_exp <- pca_var/sum(pca_var)
round(prop_var_exp, 3)*100
```

The result above shows that the first component explans more than $44\%$ variance. Second component explains $19\%$ variance and so on. The scree plot would help to select the number of componets for modelling.

```{r, echo=T}
plot(prop_var_exp, xlab = "Principal Component",
             ylab = "Proportion of Variance Explained",
             type = "b")
```

The plot above shows that approximately $10$ componets explains around `r round(sum(prop_var_exp[1:10])*100, 3)`% of the variance while $17$ components explains more than `r round(sum(prop_var_exp[1:17])*100, 3)`% of the variance. The comulative variance plot below shows a clear picture of the components.


```{r, echo=T}
plot(cumsum(prop_var_exp), xlab = "Principal Component",
             ylab = "Cumulative Proportion of Variance Explained",
             type = "b")
```


We can view the distribution of the diagnosis between the two diagnosis outcomes.

```{r}
pca_df <- pca_result$x %>% data.frame()
str(pca_df)
```


```{r}
p <- ggplot(pca_df, aes(x=PC1, y=PC2, col=working_df$diagnosis)) + geom_point(alpha=0.5) +
  labs(color = "Diagnosis")
p
```



#### LDA

Linear Discriminant Analysis (LDA) are used in ML to estimate the linear combination of features which can correctly clasify two or more clases of outcome variable. The LDA classification problem is finding a good predictor which correctly classifies the outcome variable given a feature (independent variables) froma set of features.


```{r, echo=T}
lda_result <- lda(diagnosis~., data = working_df, 
                  center=T, scale = T)
lda_result$prior
```

The lda result contains the prior probability of each diagnosis class, counts, class-specific means, singular values (svd) and so on. We can use svd to compute the amount of between-group variance that is explained by each linear discriminant.

```{r, echo=T}
prop_var <- lda_result$svd^2/sum(lda_result$svd^2)
prop_var
```

We see that the first linear discriminant explains a `r prop_var*100`% of the between-group variance in the data.


```{r, echo=T}
lda_result_df <- predict(lda_result, working_df)
lda_result_df <- lda_result_df$x %>% as.data.frame()
lda_result_df <- cbind(lda_result_df, 
                       diagnosis=working_df[, "diagnosis"])
p <- ggplot(lda_result_df, aes(x=LD1, fill=diagnosis)) + geom_density(alpha=0.5) + 
  labs(color = "Diagnosis")
p
```



### Machine Learning Models

Machine learning is a science which involves the application of Artificial Intelligence (AI) that enables the computers to automatically learn and improve or get things done based on experience without being explicitly programed. This involves computer looking for patterns based on previous observations, experiences or instructions. ML algorithms are broadly classified as supervised, unsupervised and reinforcement learning. In this exercise, we will fit some of the supervised learning algorithms:

* LDA
* Neural Networks
* K - Nearest Neighbors (KNN)
* Random Forest
* Naive Bayes
* Support Vector Machine (SVM)

We then further, to some models, incorporate either PCA or LDA to assess if there could be any improvement in model performance.

#### Dataset Partitioning 

We need to assess the model quality after model fitting. Datset partitioning helps us split the datset into training (used for model fitting) and test (used for model validation) datasets. In the present case, we split the datset into  $80$% of which we will use to train our models and $20$% that we will hold back as a validation dataset.


```{r, echo=T}
set.seed(1000)
df_index <- createDataPartition(working_df$diagnosis, p=0.7, list = F)
train_df <- working_df[df_index, -1]
test_df <- working_df[-df_index, -1]
```


Data \textbf{pre-processing} involves transforming data into a specific format to improve the performance of machine learning algorithms. We'll use \textbf{preProcess} function in package \textit{caret} to transform the data.

Our main focus is transforming our predictors (characteristics) such that we reduce their dimensionality to fewer dimension where the new characterisitcs are uncorrelated. Therefore, we'll apply \textit{pca} method with a threshold of $.99$.


```{r, echo=T}
model_control <- trainControl(method="cv",
                            number = 5,
                            preProcOptions = list(thresh = 0.99),
                            classProbs = TRUE,
                            summaryFunction = twoClassSummary)
```


#### LDA

Define training and testing data sets for LDA models.


```{r, echo=T}
train_lda_df <- lda_result_df[df_index,]
test_lda_df <- lda_result_df[-df_index,]
```


```{r, echo=T}
lda_model <- train(diagnosis~., train_lda_df,  method="lda2",  metric="ROC",
                   preProc = c("center", "scale"),
                   tuneLength = 10,
                   trControl=model_control)
```




Using \textit{predict} function, we generate the clasiffication and posterior probabilities based on the LDA model.

```{r, echo=T}
lda_predicted <- predict(lda_model, test_lda_df)
knitr::kable(as.data.frame(table(lda_predicted)))
```

We can summarize the performance of the LDA classification  using $\textit{Confusion Matrix}$.

```{r, echo=T}
lda_confusion_mat <- confusionMatrix(lda_predicted, test_lda_df$diagnosis, positive = "M")
knitr::kable(lda_confusion_mat$table)
```


The overall accuracy of our model is `r round(lda_confusion_mat$overall[1], 4)*100`%. In addition, our classifier correctly identified $\textit{M}$ `r round(lda_confusion_mat$byClass[1], 4)*100`% of the time (correctly predicting $\textit{M}$ when indeed we should). Further, the true negatives (our specificity) is `r round(lda_confusion_mat$byClass[2], 4)*100`%.


```{r, echo=T}
lda_predicted_prob <- predict(lda_model, test_lda_df, type="prob")
#colAUC(lda_predicted_prob, test_lda_df$diagnosis, plotROC=TRUE)
```

#### Neural Networks

Our second ML algorithm applied to the data is Neural Networks. A neural network model can be though of as a model consisting of an activation function. The activation function transforms inputto output through information processing unit. For this and other reasons, neural networks is considered complex and mostly oftenly regarded as a 'black box'.

```{r, echo=T}
nn_model <- train(diagnosis~.,
                    train_df,
                    method="nnet",
                    metric="ROC",
                    preProcess=c('center', 'scale'),
                    trace=FALSE,
                    tuneLength=10,
                    trControl=model_control)
```


Predicted values for Neural Network model.

```{r}
nn_predicted <- predict(nn_model, test_df)
knitr::kable(as.data.frame(table(nn_predicted)))
```


Summary based on $\textit{Confusion Matrix}$.

```{r, echo=T}
nn_confusion_mat <- confusionMatrix(nn_predicted, test_df$diagnosis, positive = "M")
knitr::kable(nn_confusion_mat$table)
```




The overall accuracy of our model is `r round(nn_confusion_mat$overall[1], 4)*100`%. In addition, our classifier correctly identified $\textit{M}$ `r round(nn_confusion_mat$byClass[1], 4)*100`% of the time (correctly predicting $\textit{M}$ when indeed we should). Further, the true negatives (our specificity) is `r round(nn_confusion_mat$byClass[2], 4)*100`%.


```{r, echo=T}
nn_predicted_prob <- predict(nn_model, test_df, type="prob")
#colAUC(nn_predicted_prob, test_df$diagnosis, plotROC=TRUE)
```


#### K-nearest Neighbors (KNN)

KNN is a ML algorithm widely used in classification problems. The algorithm goes through the datset to find the k-nearest case and classifies new cases by a majority vote of its k neighbors. The output, in case of classification problem, is the most frequent class. The similairty measure is expressed as a distance measure. These include Euclidean, Manhattan, Minkowski and Hamming distance. Or simply, once the data is stored in the system, its similarity is calculated for any input data point coming into the system.


```{r, echo=T}
knn_model <- train(diagnosis~.,
                   train_df,
                   method="knn",
                   metric="ROC",
                   preProcess = c('center', 'scale'),
                   tuneLength=10,
                   trControl=model_control)
```


Predicted values for Neural Network model.

```{r}
knn_predicted <- predict(knn_model, test_df)
knitr::kable(as.data.frame(table(knn_predicted)))
```


Summary based on $\textit{Confusion Matrix}$.

```{r, echo=T}
knn_confusion_mat <- confusionMatrix(knn_predicted, test_df$diagnosis, positive = "M")
knitr::kable(knn_confusion_mat$table)
```


The overall accuracy of our model is `r round(knn_confusion_mat$overall[1], 4)*100`%. In addition, our classifier correctly identified $\textit{M}$ `r round(knn_confusion_mat$byClass[1], 4)*100`% of the time (correctly predicting $\textit{M}$ when indeed we should). Further, the true negatives (our specificity) is `r round(knn_confusion_mat$byClass[2], 4)*100`%.


```{r, echo=T}
knn_predicted_prob <- predict(knn_model, test_df, type="prob")
#colAUC(knn_predicted_prob, test_df$diagnosis, plotROC=TRUE)
```





#### Random Forest

Random Forest is a supervised learning algorithm and can be understood as a bootstrapping algorithm with a Decision tree model, mostly, trained using 'bagging' method. In other words, Random forests built from several decision trees merged together to get a more stable and accurate predictions. It only important to note that the results tend to be more accurate with larger number of trees. Thi algorithm adds additonal randomness into the model with increasing number of trees.


```{r, echo=T}
rand_forest_model <- train(diagnosis~.,
                   train_df,
                   method="ranger",
                   metric="ROC",
                   preProcess = c('center', 'scale'),
                   tuneLength=10,
                   trControl=model_control)
```


Predicted values for random forest model.

```{r}
rand_forest_predicted <- predict(rand_forest_model, test_df)
knitr::kable(as.data.frame(table(rand_forest_predicted)))
```


Summary based on $textit{Confusion Matrix}$.

```{r, echo=T}
rand_forest_confusion_mat <- confusionMatrix(rand_forest_predicted, test_df$diagnosis, positive = "M")
knitr::kable(rand_forest_confusion_mat$table)
```


The overall accuracy of our model is `r round(rand_forest_confusion_mat$overall[1], 4)*100`%. In addition, our classifier correctly identified $\textit{M}$ `r round(rand_forest_confusion_mat$byClass[1], 4)*100`% of the time (correctly predicting $\textit{M}$ when indeed we should). Further, the true negatives (our specificity) is `r round(rand_forest_confusion_mat$byClass[2], 4)*100`%.


```{r, echo=T}
rand_forest_confusion_prob <- predict(rand_forest_model, test_df, type="prob")
#colAUC(rand_forest_confusion_prob, test_df$diagnosis, plotROC=TRUE)
```



#### Naive Bayes

Naive Bayes is a classification problem based on Bayes' theorem. It assumes that the all the dependent variables are independent of each other, which generally not true in many real-life problems. In other words, it assumes that the presence of a given feature in a given class is completely unrelated to the presence of any other features. The application of Naive Bayes model is considered less complicated, mostly application in large datasets and provides a way to calculte the posterior probabilities.


```{r, echo=T}
naive_bayes_model <- train(diagnosis~.,
                   train_df,
                   method="nb",
                   metric="ROC",
                   preProcess = c('center', 'scale'),
                   trace=F,
                   trControl=model_control)
```


Predicted values for Naive Bayes model.

```{r}
naive_bayes_predicted <- predict(naive_bayes_model, test_df)
knitr::kable(as.data.frame(table(naive_bayes_predicted)))
```


Summary based on $\textit{Confusion Matrix}$.

```{r, echo=T}
naive_bayes_confusion_mat <- confusionMatrix(naive_bayes_predicted, test_df$diagnosis, positive = "M")
knitr::kable(rand_forest_confusion_mat$table)
```


The overall accuracy of our model is `r round(naive_bayes_confusion_mat$overall[1], 4)*100`%. In addition, our classifier correctly identified $\textit{M}$ `r round(naive_bayes_confusion_mat$byClass[1], 4)*100`% of the time (correctly predicting $\textit{M}$ when indeed we should). Further, the true negatives (our specificity) is `r round(naive_bayes_confusion_mat$byClass[2], 4)*100`%.


```{r, echo=T}
naive_bayes_confusion_prob <- predict(naive_bayes_model, test_df, type="prob")
#colAUC(naive_bayes_confusion_prob, test_df$diagnosis, plotROC=TRUE)
```




#### Support Vector Machine (SVM) - with radial kernel

In this ML algorithm, the input vector is initially mapped into feature space of higher dimensionality and identifies the hyperplane that seperates the data points into two classes. The aim is to maximize the marginal distance between the decision hyperplane and closest boundary data points. In other words, we plot each data point in a n-dimensional space, where n is the number of features in the dataset. We then find the line that seperates the data into two different classes. The line should be such that the distance from the closest point in each of the two classes is maximized.

```{r, echo=T}
svm_model <- train(diagnosis~.,
                   train_df,
                   method="svmRadial",
                   metric="ROC",
                   preProcess = c('center', 'scale'),
                   trace=F,
                   trControl=model_control)
```


Predicted values for Vector Machine (SVM) - with radial kernel.

```{r}
svm_predicted <- predict(svm_model, test_df)
knitr::kable(as.data.frame(table(svm_predicted)))
```


Summary based on $\textit{Confusion Matrix}$.

```{r, echo=T}
svm_confusion_mat <- confusionMatrix(svm_predicted, test_df$diagnosis, positive = "M")
knitr::kable(svm_confusion_mat$table)
```



The overall accuracy of our model is `r round(svm_confusion_mat$overall[1], 4)*100`%. In addition, our classifier correctly identified $\textit{M}$ `r round(svm_confusion_mat$byClass[1], 4)*100`% of the time (correctly predicting $\textit{M}$ when indeed we should). Further, the true negatives (our specificity) is `r round(svm_confusion_mat$byClass[2], 4)*100`%.


```{r, echo=T}
svm_confusion_prob <- predict(svm_model, test_df, type="prob")
#colAUC(svm_confusion_prob, test_df$diagnosis, plotROC=TRUE)
```



#### Extending Models with PCA and LDA

We extend some of the models fitted above using LDA or/and PCA to assess whether there could some improvement in the models. Extended models are shown below.


##### Random Forest with PCA

We also fit random forest model with PCA.

```{r, echo=T}
rand_forest_pca_model <- train(diagnosis~.,
                   train_df,
                   method="ranger",
                   metric="ROC",
                   preProcess = c('center', 'scale', 'pca'),
                   trControl=model_control)
```


Predicted values for random forest model with PCA.

```{r}
rand_forest_pca_predicted <- predict(rand_forest_pca_model, test_df)
knitr::kable(as.data.frame(table(rand_forest_pca_predicted)))
```


Summary based on $\textit{Confusion Matrix}$.

```{r, echo=T}
rand_forest_pca_confusion_mat <- confusionMatrix(rand_forest_pca_predicted, 
                                                 test_df$diagnosis, positive = "M")
knitr::kable(rand_forest_pca_confusion_mat$table)
```


The overall accuracy of our model is `r round(rand_forest_pca_confusion_mat$overall[1], 4)*100`%. In addition, our classifier correctly identified $\textit{M}$ `r round(rand_forest_pca_confusion_mat$byClass[1], 4)*100`% of the time (correctly predicting $\textit{M}$ when indeed we should). Further, the true negatives (our specificity) is `r round(rand_forest_pca_confusion_mat$byClass[2], 4)*100`%.


```{r, echo=T}
rand_forest_pca_confusion_prob <- predict(rand_forest_pca_model, test_df, type="prob")
#colAUC(rand_forest_pca_confusion_prob, test_df$diagnosis, plotROC=TRUE)
```



##### Neural Networks with PCA

We also fit NW model with PCA.

```{r, echo=T}
nn_pca_model <- train(diagnosis~.,
                   train_df,
                   method="nnet",
                   metric="ROC",
                   preProcess = c('center', 'scale', 'pca'),
                   tuneLength=10,
                   trace=F,
                   trControl=model_control)
```


Predicted values for NW model with PCA.

```{r}
nn_pca_predicted <- predict(nn_pca_model, test_df)
knitr::kable(as.data.frame(table(nn_pca_predicted)))
```


Summary based on $textit{Confusion Matrix}$.

```{r, echo=T}
nn_pca_confusion_mat <- confusionMatrix(nn_pca_predicted, 
                                                 test_df$diagnosis, positive = "M")
knitr::kable(nn_pca_confusion_mat$table)
```

The overall accuracy of our model is `r round(nn_pca_confusion_mat$overall[1], 4)*100`%. In addition, our classifier correctly identified $\textit{M}$ `r round(nn_pca_confusion_mat$byClass[1], 4)*100`% of the time (correctly predicting $\textit{M}$ when indeed we should). Further, the true negatives (our specificity) is `r round(nn_pca_confusion_mat$byClass[2], 4)*100`%.


```{r, echo=T}
nn_pca_confusion_prob <- predict(nn_pca_model, test_df, type="prob")
#colAUC(nn_pca_confusion_prob, test_df$diagnosis, plotROC=TRUE)
```




##### Naive Bayes with LDA

We also fit Naive Bayes model with LDA.

```{r, echo=T}
nb_lda_model <- train(diagnosis~.,
                   train_lda_df,
                   method="nb",
                   metric="ROC",
                   preProcess = c('center', 'scale'),
                   trace=F,
                   trControl=model_control)
```


Predicted values for Naive Bayes model with LDA.

```{r}
nb_lda_predicted <- predict(nb_lda_model, test_lda_df)
knitr::kable(as.data.frame(table(nb_lda_predicted)))
```


Summary based on $\textit{Confusion Matrix}$.

```{r, echo=T}
nb_lda_confusion_mat <- confusionMatrix(nb_lda_predicted, 
                                                 test_df$diagnosis, positive = "M")
knitr::kable(nb_lda_confusion_mat$table)
```


The overall accuracy of our model is `r round(nb_lda_confusion_mat$overall[1], 4)*100`%. In addition, our classifier correctly identified $\textit{M}$ `r round(nb_lda_confusion_mat$byClass[1], 4)*100`% of the time (correctly predicting $\textit{M}$ when indeed we should). Further, the true negatives (our specificity) is `r round(nb_lda_confusion_mat$byClass[2], 4)*100`%.


```{r, echo=T}
nb_lda_confusion_prob <- predict(nb_lda_model, test_lda_df, type="prob")
#colAUC(nb_lda_confusion_prob, test_lda_df$diagnosis, plotROC=TRUE)
```




##### Neural Networks model with LDA

We also fit Neural Networks model with LDA.

```{r, echo=T}
nn_lda_model <- train(diagnosis~.,
                   train_lda_df,
                   method="nnet",
                   metric="ROC",
                   preProcess = c('center', 'scale'),
                   tuneLength=10,
                   trace=F,
                   trControl=model_control)
```


Predicted values for Neural Networks model with LDA.

```{r}
nn_lda_predicted <- predict(nn_lda_model, test_lda_df)
knitr::kable(as.data.frame(table(nn_lda_predicted)))
```


Summary based on $\textit{Confusion Matrix}$.

```{r, echo=T}
nn_lda_confusion_mat <- confusionMatrix(nn_lda_predicted, 
                                                 test_df$diagnosis, positive = "M")
knitr::kable(nn_lda_confusion_mat$table)
```


The overall accuracy of our model is `r round(nn_lda_confusion_mat$overall[1], 4)*100`%. In addition, our classifier correctly identified $\textit{M}$ `r round(nn_lda_confusion_mat$byClass[1], 4)*100`% of the time (correctly predicting $\textit{M}$ when indeed we should). Further, the true negatives (our specificity) is `r round(nn_lda_confusion_mat$byClass[2], 4)*100`%.


```{r, echo=T}
nn_lda_confusion_prob <- predict(nn_lda_model, test_lda_df, type="prob")
#colAUC(nn_lda_confusion_prob, test_lda_df$diagnosis, plotROC=T)
```


### Assesing Model Performance

In this Section, we describe some methods we've used in evaluating ML models fitted above. Specifically, we have:

* $\textbf{Accuracy}$ - Is the proportion (percentage) of correctly classifying all the data points.
* $\textbf{Kappa}$ - This provides the classification accuracy. It important where there are imbalances in classes.
* $\textbf{Sensitivity}$ - Is the true positive rate also called the recall. It is the number instances from the positive (first) class that actually predicted correctly.
* $\textbf{Specificity}$ - Is also called the true negative rate. Is the number of instances from the negative class (second) class that were actually predicted correctly.

We start by comparing the area under curve (AUC) of ROC curves. The AUC provides the models ability to discriminate between positive and negative classes. An AUC of $1$ implies that the model provide a perfect prediction while an AUC of 0.5 implies that the model is good as random.

```{r, echo=F}

old.par <- par(mfrow=c(2, 5))
colAUC(lda_predicted_prob, test_lda_df$diagnosis, plotROC=TRUE)
text(0.5, 0.5, "LDA", cex = 1.5)
colAUC(nn_predicted_prob, test_df$diagnosis, plotROC=TRUE)
text(0.5, 0.5, "NN", cex = 1.5)
colAUC(knn_predicted_prob, test_df$diagnosis, plotROC=TRUE)
text(0.5, 0.5, "KNN", cex = 1.5)
colAUC(rand_forest_confusion_prob, test_df$diagnosis, plotROC=TRUE)
text(0.5, 0.5, "RF", cex = 1.5)
colAUC(naive_bayes_confusion_prob, test_df$diagnosis, plotROC=TRUE)
text(0.5, 0.5, "NB", cex = 1.5)
colAUC(svm_confusion_prob, test_df$diagnosis, plotROC=TRUE)
text(0.5, 0.5, "SVM", cex = 1.5)
colAUC(rand_forest_pca_confusion_prob, test_df$diagnosis, plotROC=TRUE)
text(0.5, 0.5, "RF_PCA", cex = 1.5)
colAUC(nn_pca_confusion_prob, test_df$diagnosis, plotROC=TRUE)
text(0.5, 0.5, "NN_PCA", cex = 1.5)
colAUC(nb_lda_confusion_prob, test_df$diagnosis, plotROC=TRUE)
text(0.5, 0.5, "NB_LDA", cex = 1.5)
colAUC(nn_lda_confusion_prob, test_df$diagnosis, plotROC=TRUE)
text(0.5, 0.5, "NN_LDA", cex = 1.5)

par(old.par)

```


We further plot the correlation matrix between the models and compare their ROC.


```{r, echo=T}
models <- list(LDA=lda_model, NN=nn_model, KNN=knn_model,
                        RF = rand_forest_model, NB=naive_bayes_model, 
                        SVM=svm_model, RF_PCA=rand_forest_pca_model,
                        NN_PCA=nn_pca_model, NB_LDA=nb_lda_model, 
                        NN_LDA=nn_lda_model)
models_resamples <- resamples(models)
models_correlation <- modelCor(models_resamples)
corrplot(models_correlation)
```


We further provide visual representation of ROC. We observe high variability in some models, specifically, NB and RF_PCA. On the other hand, NN_LDA and LDA provide high AUC but with some variability.

```{r, error=T}
bwplot(models_resamples, metric="ROC")
```





#### Model Selection

LDA, NB_LDA and NN_LDA provide the highest accuracy and Kappa in comparisson to the other models.

```{r}
models_conf_mat <- list(LDA=lda_confusion_mat, NN=nn_confusion_mat, KNN=knn_confusion_mat,
                        RF = rand_forest_confusion_mat, NB=naive_bayes_confusion_mat, 
                        SVM=svm_confusion_mat, RF_PCA=rand_forest_pca_confusion_mat,
                        NN_PCA=nn_confusion_mat, NB_LDA=nb_lda_confusion_mat, 
                        NN_LDA=nn_lda_confusion_mat)

all_model_accuracy <- sapply(models_conf_mat, function(x){
  round(x$overall, 2)
})
knitr::kable(data.frame(all_model_accuracy))
```
In addition, NN_LDA model provides the highest sensitivity.

```{r, echo=T}
#function(x) x$byClass
all_model_results <- sapply(models_conf_mat, function(x){
  round(x$byClass, 2)
})
knitr::kable(data.frame(all_model_results))
```

To provide a summary of the best model, we pullout model which performs better in each metric. From the output below, we observe that the model with the highest sensitivity (detection of breast cancer cases) is NN_LDA.

```{r}
all_model_results_max <- apply(all_model_results, 1, which.is.max)

model_select <- data.frame(metric=names(all_model_results_max), 
                            best_model=colnames(all_model_results)[all_model_results_max],
                            value=mapply(function(x,y) {all_model_results[x,y]}, 
                            names(all_model_results_max), 
                            all_model_results_max))
rownames(model_select) <- NULL
names(model_select) <- c("Metric", "Best_model", "Value")
knitr::kable(data.frame(model_select))
```

### Conclusion

In conclusion, we have found that a model based on Neural Network and LDA provides a good classification of the data. This model has a sensitivity of 0.94.


